{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(message):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S -\"), message)\n",
    "    \n",
    "def printnow():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary of dataframes for training and testing\n",
    "section_headers = ['Intro','Verse','Refrain','Pre-Chorus','Chorus','Post-Chorus','Hooks','Riffs/Basslines','Scratches','Sampling','Bridge','Interlude','Skit','Collision','Instrumental','Solo','Ad-lib','Segue','Outro']\n",
    "header_strip_list = '|'.join(['\\[' + header + '\\]' for header in section_headers])\n",
    "\n",
    "def header_to_filename(train, header):\n",
    "    if train: return 'section_train_test/train_' + header.replace('/', '_').lower() + '.zip'\n",
    "    else: return 'section_train_test/test_' + header.replace('/', '_').lower() + '.zip'\n",
    "\n",
    "log('Reading data from files')\n",
    "train_dfs = {header:pd.read_pickle(header_to_filename(1, header)) for header in section_headers}\n",
    "test_dfs = {header:pd.read_pickle(header_to_filename(0, header)) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "# For dataframes without samples of each genre, add the empty string as lyrics for all genres\n",
    "dummy_df = pd.DataFrame(data={'lyrics' : 8*[''], 'genre' : 2*['country', 'hiphop', 'pop', 'rock']})\n",
    "for header in section_headers:\n",
    "    if len(train_dfs[header]) < 5: train_dfs[header] = pd.concat([train_dfs[header], dummy_df])\n",
    "    if len(test_dfs[header]) < 5: test_dfs[header] = pd.concat([test_dfs[header], dummy_df])\n",
    "    train_dfs[header] = train_dfs[header].reset_index(drop=True)\n",
    "    test_dfs[header] = test_dfs[header].reset_index(drop=True)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('Finding the counts of each section type in the corpus')\n",
    "total_length = 0\n",
    "for i,header in enumerate(section_headers):\n",
    "    length = len(train_dfs[header]) + len(test_dfs[header])\n",
    "    print(i + 1, header + ':', length)\n",
    "    total_length += len(train_dfs[header]) + len(test_dfs[header])\n",
    "log(f'Done: total length is {total_length}\\n')\n",
    "\n",
    "log('Finding the percent frequencies of each section type in the corpus')\n",
    "corpus_weights = {}\n",
    "for i,header in enumerate(section_headers):\n",
    "    frequency = (len(train_dfs[header]) + len(test_dfs[header])) / total_length\n",
    "    print(i + 1, header + ':', frequency)\n",
    "    corpus_weights[header] = frequency\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nonredundant data\n",
    "country_df = pd.read_pickle(r'train_test_data/country_data.zip')\n",
    "country_df['genre'] = 'country'\n",
    "hiphop_df = pd.read_pickle(r'train_test_data/hiphop_data.zip')\n",
    "hiphop_df['genre'] = 'hiphop'\n",
    "pop_df = pd.read_pickle(r'train_test_data/pop_data.zip')\n",
    "pop_df['genre'] = 'pop'\n",
    "rock_df = pd.read_pickle(r'train_test_data/rock_data.zip')\n",
    "rock_df['genre'] = 'rock'\n",
    "full_df = pd.concat([country_df,hiphop_df, pop_df,rock_df])\n",
    "full_df.reset_index(inplace=True, drop=True)\n",
    "full_df\n",
    "\n",
    "# Combine lyrics into one list to be input for tf-idf vectorizer WITH UNIGRAMS UP TO TRIGRAMS\n",
    "full_lyrics = [lyrics.lower() for lyrics in full_df['lyrics']]\n",
    "log(f'Training TF-IDF Vectorizer on all {len(full_lyrics)} lyrics')\n",
    "tfidf_ngram_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,3)).fit(full_lyrics)\n",
    "tfidf_ngram_features = tfidf_ngram_vectorizer.get_feature_names()\n",
    "tfidf_ngram_data = tfidf_ngram_vectorizer.transform(full_lyrics)\n",
    "log('Fitting SVD on all lyrics')\n",
    "svd_ngram = TruncatedSVD(n_components=500).fit(tfidf_ngram_data)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFIDF up to trigram encoding for nonredundant data\n",
    "def encode_tfidf_ngram_svd(df): \n",
    "    tfidf_ngram_vec = tfidf_ngram_vectorizer.transform(df['lyrics'].values)\n",
    "    svd_ngram_vec = svd_ngram.transform(tfidf_ngram_vec)\n",
    "    tfidf_ngram_df = pd.DataFrame(svd_ngram_vec)\n",
    "    tfidf_ngram_df['y'] = df['genre']\n",
    "    return tfidf_ngram_df\n",
    "\n",
    "# Create the training and testing encoded dataframes\n",
    "log('Encoding the training data')\n",
    "tfidf_train = {header:encode_tfidf_ngram_svd(train_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "log('Encoding the testing data')\n",
    "tfidf_test = {header:encode_tfidf_ngram_svd(test_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the string lyrics data to make predictions\n",
    "log('Loading the raw string data')\n",
    "string_train = pd.read_pickle(r'train_test_data/train.zip')\n",
    "string_test = pd.read_pickle(r'train_test_data/test.zip')\n",
    "log('Done\\n')\n",
    "\n",
    "# splits the given lyrics by section \n",
    "def split_by_section(lyrics):\n",
    "    headers = [word[1:-1] for word in lyrics.split() if word[0] == '[' and word[-1] == ']' and word[1:-1] in section_headers]\n",
    "    split_sections = re.split(header_strip_list, lyrics)\n",
    "    ret_sections = []\n",
    "    for section in split_sections:\n",
    "        mod_section = section.replace('[END]','').replace('[START]','').strip()\n",
    "        if not(mod_section in ['', ' ','\\n']): ret_sections.append(mod_section)\n",
    "    return list(zip(headers, ret_sections))\n",
    "\n",
    "# Turn the raw string data into tuples of section strings and the lyrics of that section\n",
    "log('Splitting the raw string data')\n",
    "split_string_train = string_train\n",
    "split_string_train['lyrics'] = string_train['lyrics'].map(split_by_section)\n",
    "split_string_test = string_test\n",
    "split_string_test['lyrics'] = string_test['lyrics'].map(split_by_section)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the split string data with TFIDF (up to trigrams) and SVD \n",
    "def split_lyrics_encode(split_lyrics):\n",
    "    sections_array = [section for section,lyrics in split_lyrics]\n",
    "    lyrics_array = [lyrics + ' ' for section,lyrics in split_lyrics]\n",
    "    try:\n",
    "        tfidf_ngram_vec = tfidf_ngram_vectorizer.transform(lyrics_array)\n",
    "        svd_ngram_vec = svd_ngram.transform(tfidf_ngram_vec)\n",
    "        assert len(svd_ngram_vec) == len(sections_array)\n",
    "        assert len(svd_ngram_vec[0]) == 500\n",
    "        return list(zip(sections_array, svd_ngram_vec))\n",
    "    except ValueError:\n",
    "        log('ValueError' + str(len(sections_array)) + '\\t' + str([len(lyrics) for lyrics in lyrics_array]))\n",
    "        return list(zip(sections_array, np.array([[0]*500]*len(split_lyrics))))\n",
    "\n",
    "# Turn the split string data into tuples of section strings and TFIDF encoded lyrics of that section\n",
    "log('Splitting the encoding string lyrics of each section')\n",
    "log('\\tTesting set')\n",
    "split_encoded_test = split_string_test\n",
    "split_encoded_test['lyrics'] = split_string_test['lyrics'].map(split_lyrics_encode)\n",
    "log('\\tTraining set')\n",
    "split_encoded_train = split_string_train\n",
    "split_encoded_train['lyrics'] = split_string_train['lyrics'].map(split_lyrics_encode)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('Saving split encoded lyrics as pickles')\n",
    "split_encoded_train.to_pickle('section_train_test/split_encoded_train.zip')\n",
    "split_encoded_test.to_pickle('section_train_test/split_encoded_test.zip')\n",
    "log('Done\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('Reading split encoded lyrics pickles into dataframes')\n",
    "split_encoded_train = pd.read_pickle('section_train_test/split_encoded_train.zip')\n",
    "split_encoded_test = pd.read_pickle('section_train_test/split_encoded_test.zip')\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionSplitClassifier:\n",
    "    def __init__(self, section_classifiers, weights):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def set_section_classifiers(self, section_classifiers):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def fit(self, X, y, section, verbose=0, keras=0):\n",
    "        if verbose: print(f'Training {section}...')\n",
    "        if keras:\n",
    "            history = self.section_classifiers[section].fit(X,y)\n",
    "        else:\n",
    "            self.section_classifiers[section] = self.section_classifiers[section].fit(X,y)\n",
    "        if verbose: print(f'Done training {section}')\n",
    "\n",
    "\n",
    "    def predict(self, X, keras=0):\n",
    "        def predict_section(self, X_section, section, keras=0):\n",
    "            def str_to_array(string, weight):\n",
    "                return float(weight) * np.array([\n",
    "                    int(string == 'country'), \n",
    "                    int(string == 'hiphop'),\n",
    "                    int(string == 'pop'), \n",
    "                    int(string == 'rock')])\n",
    "            if keras:\n",
    "                return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict([X_section])]\n",
    "            else:\n",
    "                return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict(np.reshape(X_section, (-1,500)))]\n",
    "\n",
    "        def predict_song(self, split_encoded, keras=0):\n",
    "            def array_to_str(array):\n",
    "                return ['country','hiphop','pop','rock'][np.argmax(array)]\n",
    "            pred = np.array([0,0,0,0])\n",
    "            for section,encoding in split_encoded:\n",
    "                pred = np.sum([pred, predict_section(self, encoding, section, keras=keras)], axis=0)\n",
    "            return array_to_str(pred)\n",
    "\n",
    "        preds = [predict_song(self, lyrics) for lyrics in X]\n",
    "        assert len(preds) == len(X)\n",
    "        return(preds)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the different kinds of section classifiers and their respective song classifiers\n",
    "section_rf_classifiers = {\n",
    "    header:RandomForestClassifier(criterion='entropy', ccp_alpha=0.0175) for header in section_headers}\n",
    "section_rf_classifier = SectionSplitClassifier(section_rf_classifiers, corpus_weights)\n",
    "\n",
    "section_ada_classifiers = {\n",
    "    header:OneVsRestClassifier(AdaBoostClassifier(), n_jobs=-1) for header in section_headers}\n",
    "section_ada_classifier = SectionSplitClassifier(section_ada_classifiers, corpus_weights)\n",
    "\n",
    "section_svm_classifiers = {\n",
    "    header:OneVsRestClassifier(SVC(kernel=\"linear\", C=0.025), n_jobs=-1) for header in section_headers}\n",
    "section_svm_classifier = SectionSplitClassifier(section_svm_classifiers, corpus_weights)\n",
    "\n",
    "section_knn_classifiers = {\n",
    "    header:OneVsOneClassifier(KNeighborsClassifier(3)) for header in section_headers}\n",
    "section_knn_classifier = SectionSplitClassifier(section_knn_classifiers, corpus_weights)\n",
    "\n",
    "# define baseline Keras neuralnet model\n",
    "def baseline_model(input_dim):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(int(input_dim/2), input_dim=input_dim, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(4, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "section_net_classifiers = {\n",
    "    header:KerasClassifier(build_fn=baseline_model, input_dim=500, epochs=20, batch_size=1, verbose=0) for header in section_headers}\n",
    "section_net_classifier = SectionSplitClassifier(section_net_classifiers, corpus_weights)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train each kind of song classifier\n",
    "# log('Training Random Forest classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_rf_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=1)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training ADA Boost classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_ada_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=1)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training Linear SVM classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_svm_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=1)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training k-Nearest Neighbors classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_knn_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=1)\n",
    "# log('Done\\n')\n",
    "\n",
    "log('Training Neural Net classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_net_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=1, keras=1)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_write_results(filename, classifier_name, classifier, X_train, X_test, verbose=1):\n",
    "    results_file = open(filename, 'a')\n",
    "    if verbose: log(f'Predicting results for {classifier_name}')\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    if verbose: log('Done\\n')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "        print(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "        print(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "        print(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "        print(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "        print(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "\n",
    "    results_file.write(f'Results for {classifier_name}\\n')\n",
    "    results_file.write(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    results_file.write(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    results_file.write(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    results_file.write(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "    results_file.write(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    results_file.write(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "    results_file.write(str('-' * 30) + '\\n')\n",
    "    results_file.close()\n",
    "\n",
    "    return y_pred_train, y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the genres of both training and testing sets\n",
    "log('start')\n",
    "y_pred_train_rf, y_pred_test_rf = predict_write_results('eval_tfidf_section.txt', 'Random Forest Classifier', section_rf_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('rf prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_ada, y_pred_test_ada = predict_write_results('eval_tfidf_section.txt', 'ADA Boost Classifier', section_ada_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('ada prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_svm, y_pred_test_svm = predict_write_results('eval_tfidf_section.txt', 'Linear SVM Classifier', section_svm_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('svm prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_knn, y_pred_test_knn = predict_write_results('eval_tfidf_section.txt', 'k-Nearest Neighbor Classifier', section_knn_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('knn prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_encoded_train['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_net, y_pred_test_net = predict_write_results('eval_tfidf_section.txt', 'Neural Net Classifier', section_net_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('net prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_net_classifier.section_classifiers['Intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train each kind of song classifier on WHOLE CORPUS \n",
    "# to find an UPPER BOUND performance metric\n",
    "tfidf_full = {header:pd.concat([tfidf_train[header], tfidf_test[header]]) for header in section_headers}\n",
    "\n",
    "log('Training Random Forest classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_rf_classifier.fit(tfidf_full[header].drop(columns=['y']), tfidf_full[header]['y'], header, verbose=1)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training ADA Boost classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_ada_classifier.fit(tfidf_full[header].drop(columns=['y']), tfidf_full[header]['y'], header, verbose=1)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training Linear SVM classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_svm_classifier.fit(tfidf_full[header].drop(columns=['y']), tfidf_full[header]['y'], header, verbose=1)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training k-Nearest Neighbors classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_knn_classifier.fit(tfidf_full[header].drop(columns=['y']), tfidf_full[header]['y'], header, verbose=1)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = open('eval_tfidf_section.txt', 'a')\n",
    "results_file.write(str('-' * 30) + '\\n')\n",
    "results_file.write(str('-' * 30) + '\\n')\n",
    "results_file.write('\\nTRAINING AND TESTING USING WHOLE CORPUS TO GET UPPERBOUND PERFORMANCE\\nRESULTS BELOW\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_rf, y_pred_test_rf = predict_write_results('eval_tfidf_section.txt', 'Random Forest Classifier', section_rf_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('rf prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_ada, y_pred_test_ada = predict_write_results('eval_tfidf_section.txt', 'ADA Boost Classifier', section_ada_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('ada prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_svm, y_pred_test_svm = predict_write_results('eval_tfidf_section.txt', 'Linear SVM Classifier', section_svm_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('svm prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_knn, y_pred_test_knn = predict_write_results('eval_tfidf_section.txt', 'k-Nearest Neighbor Classifier', section_knn_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('knn prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}