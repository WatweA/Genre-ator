{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(message):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S -\"), message)\n",
    "    \n",
    "def printnow():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "15:20:32 - Reading data from files\n15:20:32 - Done\n\n"
    }
   ],
   "source": [
    "# Create the dictionary of dataframes for training and testing\n",
    "section_headers = ['Intro','Verse','Refrain','Pre-Chorus','Chorus','Post-Chorus','Hooks','Riffs/Basslines','Scratches','Sampling','Bridge','Interlude','Skit','Collision','Instrumental','Solo','Ad-lib','Segue','Outro']\n",
    "header_strip_list = '|'.join(['\\[' + header + '\\]' for header in section_headers])\n",
    "\n",
    "def header_to_filename(train, header):\n",
    "    if train: return 'section_train_test/train_' + header.replace('/', '_').lower() + '.zip'\n",
    "    else: return 'section_train_test/test_' + header.replace('/', '_').lower() + '.zip'\n",
    "\n",
    "log('Reading data from files')\n",
    "train_dfs = {header:pd.read_pickle(header_to_filename(1, header)) for header in section_headers}\n",
    "test_dfs = {header:pd.read_pickle(header_to_filename(0, header)) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "# For dataframes without samples of each genre, add the empty string as lyrics for all genres\n",
    "dummy_df = pd.DataFrame(data={'lyrics' : 8*[''], 'genre' : 2*['country', 'hiphop', 'pop', 'rock']})\n",
    "for header in section_headers:\n",
    "    if len(train_dfs[header]) < 5: train_dfs[header] = pd.concat([train_dfs[header], dummy_df])\n",
    "    if len(test_dfs[header]) < 5: test_dfs[header] = pd.concat([test_dfs[header], dummy_df])\n",
    "    train_dfs[header] = train_dfs[header].reset_index(drop=True)\n",
    "    test_dfs[header] = test_dfs[header].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "15:20:32 - Finding the counts of each section type in the corpus\n1 Intro: 1056\n2 Verse: 11248\n3 Refrain: 438\n4 Pre-Chorus: 4128\n5 Chorus: 13602\n6 Post-Chorus: 1029\n7 Hooks: 32\n8 Riffs/Basslines: 32\n9 Scratches: 32\n10 Sampling: 32\n11 Bridge: 3417\n12 Interlude: 259\n13 Skit: 33\n14 Collision: 34\n15 Instrumental: 190\n16 Solo: 177\n17 Ad-lib: 35\n18 Segue: 32\n19 Outro: 2332\n15:20:32 - Done: total length is 38138\n\n15:20:32 - Finding the percent frequencies of each section type in the corpus\n1 Intro: 0.027688919188211234\n2 Verse: 0.29492894226231053\n3 Refrain: 0.011484608526928522\n4 Pre-Chorus: 0.10823850228118936\n5 Chorus: 0.3566521579526981\n6 Post-Chorus: 0.026980963868058105\n7 Hooks: 0.0008390581572185222\n8 Riffs/Basslines: 0.0008390581572185222\n9 Scratches: 0.0008390581572185222\n10 Sampling: 0.0008390581572185222\n11 Bridge: 0.08959567885049033\n12 Interlude: 0.006791126959987414\n13 Skit: 0.0008652787246316011\n14 Collision: 0.0008914992920446798\n15 Instrumental: 0.004981907808484976\n16 Solo: 0.004641040432114951\n17 Ad-lib: 0.0009177198594577587\n18 Segue: 0.0008390581572185222\n19 Outro: 0.061146363207299805\n15:20:32 - Done\n\n"
    }
   ],
   "source": [
    "log('Finding the counts of each section type in the corpus')\n",
    "total_length = 0\n",
    "for i,header in enumerate(section_headers):\n",
    "    length = len(train_dfs[header]) + len(test_dfs[header])\n",
    "    print(i + 1, header + ':', length)\n",
    "    total_length += len(train_dfs[header]) + len(test_dfs[header])\n",
    "log(f'Done: total length is {total_length}\\n')\n",
    "\n",
    "log('Finding the percent frequencies of each section type in the corpus')\n",
    "corpus_weights = {}\n",
    "for i,header in enumerate(section_headers):\n",
    "    frequency = (len(train_dfs[header]) + len(test_dfs[header])) / total_length\n",
    "    print(i + 1, header + ':', frequency)\n",
    "    corpus_weights[header] = frequency\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "15:20:33 - Training TF-IDF Vectorizer on all 4921 lyrics\n15:20:47 - Fitting SVD on all lyrics\n15:22:41 - Done\n\n"
    }
   ],
   "source": [
    "# Load nonredundant data\n",
    "country_df = pd.read_pickle(r'train_test_data/country_data.zip')\n",
    "country_df['genre'] = 'country'\n",
    "hiphop_df = pd.read_pickle(r'train_test_data/hiphop_data.zip')\n",
    "hiphop_df['genre'] = 'hiphop'\n",
    "pop_df = pd.read_pickle(r'train_test_data/pop_data.zip')\n",
    "pop_df['genre'] = 'pop'\n",
    "rock_df = pd.read_pickle(r'train_test_data/rock_data.zip')\n",
    "rock_df['genre'] = 'rock'\n",
    "full_df = pd.concat([country_df,hiphop_df, pop_df,rock_df])\n",
    "full_df.reset_index(inplace=True, drop=True)\n",
    "full_df\n",
    "\n",
    "# Combine lyrics into one list to be input for tf-idf vectorizer WITH UNIGRAMS UP TO TRIGRAMS\n",
    "full_lyrics = [lyrics.lower() for lyrics in full_df['lyrics']]\n",
    "log(f'Training TF-IDF Vectorizer on all {len(full_lyrics)} lyrics')\n",
    "tfidf_ngram_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,3)).fit(full_lyrics)\n",
    "tfidf_ngram_features = tfidf_ngram_vectorizer.get_feature_names()\n",
    "tfidf_ngram_data = tfidf_ngram_vectorizer.transform(full_lyrics)\n",
    "log('Fitting SVD on all lyrics')\n",
    "svd_ngram = TruncatedSVD(n_components=500).fit(tfidf_ngram_data)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "15:22:41 - Encoding the training data\n15:23:07 - Done\n\n15:23:07 - Encoding the testing data\n15:23:29 - Done\n\n"
    }
   ],
   "source": [
    "# Create TFIDF up to trigram encoding for nonredundant data\n",
    "def encode_tfidf_ngram_svd(df): \n",
    "    tfidf_ngram_vec = tfidf_ngram_vectorizer.transform(df['lyrics'].values)\n",
    "    svd_ngram_vec = svd_ngram.transform(tfidf_ngram_vec)\n",
    "    tfidf_ngram_df = pd.DataFrame(svd_ngram_vec)\n",
    "    tfidf_ngram_df['y'] = df['genre']\n",
    "    return tfidf_ngram_df\n",
    "\n",
    "# Create the training and testing encoded dataframes\n",
    "log('Encoding the training data')\n",
    "tfidf_train = {header:encode_tfidf_ngram_svd(train_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "log('Encoding the testing data')\n",
    "tfidf_test = {header:encode_tfidf_ngram_svd(test_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "15:23:29 - Loading the raw string data\n15:23:29 - Done\n\n15:23:29 - Splitting the raw string data\n15:23:30 - Done\n\n"
    }
   ],
   "source": [
    "# Load the string lyrics data to make predictions\n",
    "log('Loading the raw string data')\n",
    "string_train = pd.read_pickle(r'train_test_data/train.zip')\n",
    "string_test = pd.read_pickle(r'train_test_data/test.zip')\n",
    "log('Done\\n')\n",
    "\n",
    "# splits the given lyrics by section \n",
    "def split_by_section(lyrics):\n",
    "    headers = [word[1:-1] for word in lyrics.split() if word[0] == '[' and word[-1] == ']' and word[1:-1] in section_headers]\n",
    "    split_sections = re.split(header_strip_list, lyrics)\n",
    "    ret_sections = []\n",
    "    for section in split_sections:\n",
    "        mod_section = section.replace('[END]','').replace('[START]','').strip()\n",
    "        if not(mod_section in ['', ' ','\\n']): ret_sections.append(mod_section)\n",
    "    return list(zip(headers, ret_sections))\n",
    "\n",
    "# Turn the raw string data into tuples of section strings and the lyrics of that section\n",
    "log('Splitting the raw string data')\n",
    "split_string_train = string_train\n",
    "split_string_train['lyrics'] = string_train['lyrics'].map(split_by_section)\n",
    "split_string_test = string_test\n",
    "split_string_test['lyrics'] = string_test['lyrics'].map(split_by_section)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16:06:23 - Splitting the encoding string lyrics of each section\n16:06:23 - \tTesting set\n16:22:34 - \tTraining set\n16:31:18 - ValueError0\t[]\n16:42:28 - ValueError0\t[]\n17:11:53 - ValueError0\t[]\n17:29:41 - ValueError0\t[]\n17:31:14 - Done\n\n"
    }
   ],
   "source": [
    "# Encode the split string data with TFIDF (up to trigrams) and SVD \n",
    "def split_lyrics_encode(split_lyrics):\n",
    "    sections_array = [section for section,lyrics in split_lyrics]\n",
    "    lyrics_array = [lyrics + ' ' for section,lyrics in split_lyrics]\n",
    "    try:\n",
    "        tfidf_ngram_vec = tfidf_ngram_vectorizer.transform(lyrics_array)\n",
    "        svd_ngram_vec = svd_ngram.transform(tfidf_ngram_vec)\n",
    "        assert len(svd_ngram_vec) == len(sections_array)\n",
    "        assert len(svd_ngram_vec[0]) == 500\n",
    "        return list(zip(sections_array, svd_ngram_vec))\n",
    "    except ValueError:\n",
    "        log('ValueError' + str(len(sections_array)) + '\\t' + str([len(lyrics) for lyrics in lyrics_array]))\n",
    "        return list(zip(sections_array, np.array([[0]*500]*len(split_lyrics))))\n",
    "\n",
    "# Turn the split string data into tuples of section strings and TFIDF encoded lyrics of that section\n",
    "log('Splitting the encoding string lyrics of each section')\n",
    "log('\\tTesting set')\n",
    "split_encoded_test = split_string_test\n",
    "split_encoded_test['lyrics'] = split_string_test['lyrics'].map(split_lyrics_encode)\n",
    "log('\\tTraining set')\n",
    "split_encoded_train = split_string_train\n",
    "split_encoded_train['lyrics'] = split_string_train['lyrics'].map(split_lyrics_encode)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17:42:25 - Saving split encoded lyrics as pickles\n17:42:32 - Done\n\n"
    }
   ],
   "source": [
    "log('Saving split encoded lyrics as pickles')\n",
    "split_encoded_train.to_pickle('section_train_test/split_encoded_train.zip')\n",
    "split_encoded_test.to_pickle('section_train_test/split_encoded_test.zip')\n",
    "log('Done\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17:44:06 - Reading split encoded lyrics pickles into dataframes\n17:44:08 - Done\n\n"
    }
   ],
   "source": [
    "log('Reading split encoded lyrics pickles into dataframes')\n",
    "split_encoded_train = pd.read_pickle('section_train_test/split_encoded_train.zip')\n",
    "split_encoded_test = pd.read_pickle('section_train_test/split_encoded_test.zip')\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionSplitClassifier:\n",
    "    def __init__(self, section_classifiers, weights):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def set_section_classifiers(self, section_classifiers):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def fit(self, X, y, section, verbose=0):\n",
    "        if verbose: print(f'Training {section}...')\n",
    "        self.section_classifiers[section] = self.section_classifiers[section].fit(X,y)\n",
    "        if verbose: print(f'Done training {section}')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        def predict_section(self, X_section, section):\n",
    "            def str_to_array(string, weight):\n",
    "                return float(weight) * np.array([\n",
    "                    int(string == 'country'), \n",
    "                    int(string == 'hiphop'),\n",
    "                    int(string == 'pop'), \n",
    "                    int(string == 'rock')])\n",
    "            return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict([X_section])]\n",
    "\n",
    "        def predict_song(self, split_encoded):\n",
    "            def array_to_str(array):\n",
    "                return ['country','hiphop','pop','rock'][np.argmax(array)]\n",
    "            pred = np.array([0,0,0,0])\n",
    "            for section,encoding in split_encoded:\n",
    "                pred = np.sum([pred, predict_section(self, encoding, section)], axis=0)\n",
    "            return array_to_str(pred)\n",
    "\n",
    "        preds = [predict_song(self, lyrics) for lyrics in X]\n",
    "        assert len(preds) == len(X)\n",
    "        return(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create the different kinds of section classifiers and their respective song classifiers\n",
    "section_rf_classifiers = {\n",
    "    header:RandomForestClassifier(criterion='entropy', ccp_alpha=0.0175) for header in section_headers}\n",
    "section_rf_classifier = SectionSplitClassifier(section_rf_classifiers, corpus_weights)\n",
    "\n",
    "section_ada_classifiers = {\n",
    "    header:OneVsRestClassifier(AdaBoostClassifier(), n_jobs=-1) for header in section_headers}\n",
    "section_ada_classifier = SectionSplitClassifier(section_ada_classifiers, corpus_weights)\n",
    "\n",
    "section_svm_classifiers = {\n",
    "    header:OneVsRestClassifier(SVC(kernel=\"linear\", C=0.025), n_jobs=-1) for header in section_headers}\n",
    "section_svm_classifier = SectionSplitClassifier(section_svm_classifiers, corpus_weights)\n",
    "\n",
    "section_knn_classifiers = {\n",
    "    header:OneVsOneClassifier(KNeighborsClassifier(3)) for header in section_headers}\n",
    "section_knn_classifier = SectionSplitClassifier(section_knn_classifiers, corpus_weights)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train each kind of song classifier\n",
    "log('Training Random Forest classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_rf_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training ADA Boost classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_ada_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training Linear SVM classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_svm_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training k-Nearest Neighbors classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_knn_classifier.fit(tfidf_train[header].drop(columns=['y']), tfidf_train[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# predict the genres of both training and testing sets\n",
    "y_pred_train = section_split_classifier2.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test = section_split_classifier2.predict(split_encoded_test['lyrics'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Set Accuracy: 0.5111788617886179\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.72      0.29      0.41       738\n      hiphop       0.48      0.85      0.61      1131\n         pop       0.51      0.86      0.64       973\n        rock       0.64      0.01      0.02      1094\n\n    accuracy                           0.51      3936\n   macro avg       0.59      0.50      0.42      3936\nweighted avg       0.58      0.51      0.42      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[211  41 486   0]\n [ 12 958 157   4]\n [ 59  79 834   1]\n [ 11 919 155   9]]\n\nTesting Set Accuracy: 0.46294416243654823\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.71      0.24      0.36       199\n      hiphop       0.42      0.83      0.56       255\n         pop       0.48      0.82      0.60       239\n        rock       0.00      0.00      0.00       292\n\n    accuracy                           0.46       985\n   macro avg       0.40      0.47      0.38       985\nweighted avg       0.37      0.46      0.36       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[ 48  13 138   0]\n [  0 212  38   5]\n [ 19  24 196   0]\n [  1 251  40   0]]\n\n"
    }
   ],
   "source": [
    "print(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "print(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "print('-' * 20)\n",
    "print(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "\n",
    "print(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "print(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "print('-' * 20)\n",
    "print(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}