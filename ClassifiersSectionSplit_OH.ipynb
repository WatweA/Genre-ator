{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(message):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S -\"), message)\n",
    "    \n",
    "def printnow():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21:09:11 - Reading data from files\n21:09:11 - Done\n\n"
    }
   ],
   "source": [
    "# Create the dictionary of dataframes for training and testing\n",
    "section_headers = ['Intro','Verse','Refrain','Pre-Chorus','Chorus','Post-Chorus','Hooks','Riffs/Basslines','Scratches','Sampling','Bridge','Interlude','Skit','Collision','Instrumental','Solo','Ad-lib','Segue','Outro']\n",
    "header_strip_list = '|'.join(['\\[' + header + '\\]' for header in section_headers])\n",
    "\n",
    "def header_to_filename(train, header):\n",
    "    if train: return 'section_train_test/train_' + header.replace('/', '_').lower() + '.zip'\n",
    "    else: return 'section_train_test/test_' + header.replace('/', '_').lower() + '.zip'\n",
    "\n",
    "log('Reading data from files')\n",
    "train_dfs = {header:pd.read_pickle(header_to_filename(1, header)) for header in section_headers}\n",
    "test_dfs = {header:pd.read_pickle(header_to_filename(0, header)) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "# For dataframes without samples of each genre, add the empty string as lyrics for all genres\n",
    "dummy_df = pd.DataFrame(data={'lyrics' : 8*[''], 'genre' : 2*['country', 'hiphop', 'pop', 'rock']})\n",
    "for header in section_headers:\n",
    "    if len(train_dfs[header]) < 5: train_dfs[header] = pd.concat([train_dfs[header], dummy_df])\n",
    "    if len(test_dfs[header]) < 5: test_dfs[header] = pd.concat([test_dfs[header], dummy_df])\n",
    "    train_dfs[header] = train_dfs[header].reset_index(drop=True)\n",
    "    test_dfs[header] = test_dfs[header].reset_index(drop=True)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21:09:11 - Finding the counts of each section type in the corpus\n1 Intro: 1056\n2 Verse: 11248\n3 Refrain: 438\n4 Pre-Chorus: 4128\n5 Chorus: 13602\n6 Post-Chorus: 1029\n7 Hooks: 16\n8 Riffs/Basslines: 16\n9 Scratches: 16\n10 Sampling: 16\n11 Bridge: 3417\n12 Interlude: 259\n13 Skit: 17\n14 Collision: 18\n15 Instrumental: 190\n16 Solo: 177\n17 Ad-lib: 19\n18 Segue: 16\n19 Outro: 2332\n21:09:11 - Done: total length is 38010\n\n21:09:11 - Finding the percent frequencies of each section type in the corpus\n1 Intro: 0.027782162588792424\n2 Verse: 0.2959221257563799\n3 Refrain: 0.011523283346487766\n4 Pre-Chorus: 0.10860299921073402\n5 Chorus: 0.3578531965272297\n6 Post-Chorus: 0.02707182320441989\n7 Hooks: 0.0004209418574059458\n8 Riffs/Basslines: 0.0004209418574059458\n9 Scratches: 0.0004209418574059458\n10 Sampling: 0.0004209418574059458\n11 Bridge: 0.0898973954222573\n12 Interlude: 0.006813996316758748\n13 Skit: 0.00044725072349381743\n14 Collision: 0.00047355958958168905\n15 Instrumental: 0.004998684556695606\n16 Solo: 0.004656669297553276\n17 Ad-lib: 0.0004998684556695607\n18 Segue: 0.0004209418574059458\n19 Outro: 0.0613522757169166\n21:09:11 - Done\n\n"
    }
   ],
   "source": [
    "log('Finding the counts of each section type in the corpus')\n",
    "total_length = 0\n",
    "for i,header in enumerate(section_headers):\n",
    "    length = len(train_dfs[header]) + len(test_dfs[header])\n",
    "    print(i + 1, header + ':', length)\n",
    "    total_length += len(train_dfs[header]) + len(test_dfs[header])\n",
    "log(f'Done: total length is {total_length}\\n')\n",
    "\n",
    "log('Finding the percent frequencies of each section type in the corpus')\n",
    "corpus_weights = {}\n",
    "for i,header in enumerate(section_headers):\n",
    "    frequency = (len(train_dfs[header]) + len(test_dfs[header])) / total_length\n",
    "    print(i + 1, header + ':', frequency)\n",
    "    corpus_weights[header] = frequency\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#get one array of sections \n",
    "train_arr = [lyric for df in train_dfs.values() for lyric in df['lyrics']]\n",
    "\n",
    "vectorizer_OH = CountVectorizer(min_df = 3, stop_words = 'english')\n",
    "vectorizer_OH.fit(train_arr)\n",
    "vocab_size = len(vectorizer_OH.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21:09:34 - Encoding the training data\n21:09:35 - Done\n\n21:09:35 - Encoding the testing data\n21:09:35 - Done\n\n"
    }
   ],
   "source": [
    "# Create TFIDF up to trigram encoding for nonredundant data\n",
    "def encode_OH(df): \n",
    "    #vocab_length = len(vectorizer_OH.get_feature_names())\n",
    "    encoding_OH = vectorizer_OH.transform(df['lyrics'].values)\n",
    "    df_OH = pd.DataFrame(encoding_OH.toarray())\n",
    "    df_OH['y'] = df['genre']\n",
    "    return df_OH\n",
    "\n",
    "# Create the training and testing encoded dataframes\n",
    "log('Encoding the training data')\n",
    "train_OH = {header:encode_OH(train_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "log('Encoding the testing data')\n",
    "test_OH = {header:encode_OH(test_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:36:26 - Loading the raw string data\n01:36:26 - Done\n\n01:36:26 - Splitting the raw string data\n01:36:26 - Done\n\n"
    }
   ],
   "source": [
    "# Load the string lyrics data to make predictions\n",
    "log('Loading the raw string data')\n",
    "string_train = pd.read_pickle(r'train_test_data/train.zip')\n",
    "string_test = pd.read_pickle(r'train_test_data/test.zip')\n",
    "log('Done\\n')\n",
    "\n",
    "# splits the given lyrics by section \n",
    "def split_by_section(lyrics):\n",
    "    headers = [word[1:-1] for word in lyrics.split() if word[0] == '[' and word[-1] == ']' and word[1:-1] in section_headers]\n",
    "    split_sections = re.split(header_strip_list, lyrics)\n",
    "    ret_sections = []\n",
    "    for section in split_sections:\n",
    "        mod_section = section.replace('[END]','').replace('[START]','').strip()\n",
    "        if not(mod_section in ['', ' ','\\n']): ret_sections.append(mod_section)\n",
    "    return list(zip(headers, ret_sections))\n",
    "\n",
    "# Turn the raw string data into tuples of section strings and the lyrics of that section\n",
    "log('Splitting the raw string data')\n",
    "split_string_train = string_train\n",
    "split_string_train['lyrics'] = string_train['lyrics'].map(split_by_section)\n",
    "split_string_test = string_test\n",
    "split_string_test['lyrics'] = string_test['lyrics'].map(split_by_section)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:36:34 - Splitting the encoding string lyrics of each section\n01:36:34 - \tTesting set\n01:36:35 - \tTraining set\n01:36:36 - ValueError0\t[]\n01:36:36 - ValueError0\t[]\n01:36:38 - ValueError0\t[]\n01:36:38 - ValueError0\t[]\n01:36:38 - Done\n\n"
    }
   ],
   "source": [
    "# Encode the split string data with TFIDF (up to trigrams) and SVD \n",
    "def split_lyrics_encode(split_lyrics):\n",
    "    sections_array = [section for section,lyrics in split_lyrics]\n",
    "    lyrics_array = [lyrics + ' ' for section,lyrics in split_lyrics]\n",
    "    try:\n",
    "        vec_OH = vectorizer_OH.transform(lyrics_array).toarray()\n",
    "        assert len(vec_OH) == len(sections_array)\n",
    "        assert len(vec_OH[0]) == vocab_size\n",
    "        return list(zip(sections_array, vec_OH))\n",
    "    except IndexError:\n",
    "        log('ValueError' + str(len(sections_array)) + '\\t' + str([len(lyrics) for lyrics in lyrics_array]))\n",
    "        return list(zip(sections_array, np.array([[0]*vocab_size]*len(split_lyrics))))\n",
    "\n",
    "# Turn the split string data into tuples of section strings and TFIDF encoded lyrics of that section\n",
    "log('Splitting the encoding string lyrics of each section')\n",
    "log('\\tTesting set')\n",
    "split_encoded_test = split_string_test\n",
    "split_encoded_test['lyrics'] = split_string_test['lyrics'].map(split_lyrics_encode)\n",
    "log('\\tTraining set')\n",
    "split_encoded_train = split_string_train\n",
    "split_encoded_train['lyrics'] = split_string_train['lyrics'].map(split_lyrics_encode)\n",
    "log('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:37:18 - Saving split encoded lyrics as pickles\n01:38:07 - Done\n\n"
    }
   ],
   "source": [
    "log('Saving split encoded lyrics as pickles')\n",
    "split_encoded_train.to_pickle('section_train_test/OH_encoded_train.zip')\n",
    "split_encoded_test.to_pickle('section_train_test/OH_encoded_test.zip')\n",
    "log('Done\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21:09:44 - Reading split encoded lyrics pickles into dataframes\n21:11:27 - Done\n\n"
    }
   ],
   "source": [
    "log('Reading split encoded lyrics pickles into dataframes')\n",
    "split_encoded_train = pd.read_pickle('section_train_test/OH_encoded_train.zip')\n",
    "split_encoded_test = pd.read_pickle('section_train_test/OH_encoded_test.zip')\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionSplitClassifier:\n",
    "    def __init__(self, section_classifiers, weights):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def set_section_classifiers(self, section_classifiers):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def fit(self, X, y, section, verbose=0, keras=0):\n",
    "        if verbose: print(f'Training {section}...')\n",
    "        if keras:\n",
    "            history = self.section_classifiers[section].fit(X,y)\n",
    "        else:\n",
    "            self.section_classifiers[section] = self.section_classifiers[section].fit(X,y)\n",
    "        if verbose: print(f'Done training {section}')\n",
    "\n",
    "\n",
    "    def predict(self, X, keras=0):\n",
    "        def predict_section(self, X_section, section, keras=0):\n",
    "            def str_to_array(string, weight):\n",
    "                return float(weight) * np.array([\n",
    "                    int(string == 'country'), \n",
    "                    int(string == 'hiphop'),\n",
    "                    int(string == 'pop'), \n",
    "                    int(string == 'rock')])\n",
    "            if keras:\n",
    "                return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict([X_section])]\n",
    "            else:\n",
    "                return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict(np.reshape(X_section, (-1,500)))]\n",
    "\n",
    "        def predict_song(self, split_encoded, keras=0):\n",
    "            def array_to_str(array):\n",
    "                return ['country','hiphop','pop','rock'][np.argmax(array)]\n",
    "            pred = np.array([0,0,0,0])\n",
    "            for section,encoding in split_encoded:\n",
    "                pred = np.sum([pred, predict_section(self, encoding, section, keras=keras)], axis=0)\n",
    "            return array_to_str(pred)\n",
    "\n",
    "        preds = [predict_song(self, lyrics) for lyrics in X]\n",
    "        assert len(preds) == len(X)\n",
    "        return(preds)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the different kinds of section classifiers and their respective song classifiers\n",
    "section_rf_classifiers = {\n",
    "    header:RandomForestClassifier(criterion='entropy', ccp_alpha=0.0175) for header in section_headers}\n",
    "section_rf_classifier = SectionSplitClassifier(section_rf_classifiers, corpus_weights)\n",
    "\n",
    "section_ada_classifiers = {\n",
    "    header:OneVsRestClassifier(AdaBoostClassifier(), n_jobs=-1) for header in section_headers}\n",
    "section_ada_classifier = SectionSplitClassifier(section_ada_classifiers, corpus_weights)\n",
    "\n",
    "section_svm_classifiers = {\n",
    "    header:OneVsRestClassifier(SVC(kernel=\"linear\", C=0.025), n_jobs=-1) for header in section_headers}\n",
    "section_svm_classifier = SectionSplitClassifier(section_svm_classifiers, corpus_weights)\n",
    "\n",
    "section_knn_classifiers = {\n",
    "    header:OneVsOneClassifier(KNeighborsClassifier(3)) for header in section_headers}\n",
    "section_knn_classifier = SectionSplitClassifier(section_knn_classifiers, corpus_weights)\n",
    "\n",
    "# define baseline Keras neuralnet model\n",
    "def baseline_model(input_dim):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(int(input_dim/2), input_dim=input_dim, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(4, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "section_net_classifiers = {\n",
    "    header:KerasClassifier(build_fn=baseline_model, input_dim=8642, epochs=20, batch_size=100, verbose=0) for header in section_headers}\n",
    "section_net_classifier = SectionSplitClassifier(section_net_classifiers, corpus_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "21:21:32 - Training Neural Net classifier with all sections\nTraining Intro...\nEpoch 1/20\n849/849 [==============================] - 6s 7ms/step - loss: 1.3149 - accuracy: 0.3793\nEpoch 2/20\n849/849 [==============================] - 6s 7ms/step - loss: 1.0886 - accuracy: 0.5112\nEpoch 3/20\n849/849 [==============================] - 6s 7ms/step - loss: 0.9144 - accuracy: 0.5795\nEpoch 4/20\n600/849 [====================>.........] - ETA: 1s - loss: 0.7934 - accuracy: 0.6083"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7d36476b49ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Neural Net classifier with all sections'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msection_headers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msection_net_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_OH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_OH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a6aa4b3b81da>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, section, verbose, keras)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_classifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_classifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_classifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Done training {section}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train each kind of song classifier\n",
    "# log('Training Random Forest classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_rf_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training ADA Boost classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_ada_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training Linear SVM classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_svm_classifier.fit(train_OH[header]s.drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "# log('Done\\n')\n",
    "\n",
    "# log('Training k-Nearest Neighbors classifier with all sections')\n",
    "# for header in section_headers:\n",
    "#     section_knn_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "# log('Done\\n')\n",
    "\n",
    "log('Training Neural Net classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_net_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=1)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_write_results(filename, classifier_name, classifier, X_train, X_test, verbose=1):\n",
    "    results_file = open(filename, 'a')\n",
    "    if verbose: log(f'Predicting results for {classifier_name}')\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    if verbose: log('Done\\n')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "        print(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "        print(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "        print(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "        print(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "        print(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "\n",
    "    results_file.write(f'Results for {classifier_name}\\n')\n",
    "    results_file.write(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    results_file.write(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    results_file.write(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    results_file.write(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "    results_file.write(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    results_file.write(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "    results_file.write(str('-' * 30) + '\\n')\n",
    "    results_file.close()\n",
    "\n",
    "    return y_pred_train, y_pred_test\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log('start')\n",
    "y_pred_train_net, y_pred_test_net = predict_write_results('eval_onehot_section.txt', 'Neural Net Classifier', section_net_classifier, split_encoded_train['lyrics'], split_encoded_test['lyrics'], verbose=1)\n",
    "log('net prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "02:53:29 - start\n02:59:29 - rf prediction done\n04:08:51 - ada prediction done\n05:51:27 - svm prediction done\n11:10:01 - knn prediction done\n"
    }
   ],
   "source": [
    "# predict the genres of both training and testing sets\n",
    "log('start')\n",
    "y_pred_train_rf = section_rf_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_rf = section_rf_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('rf prediction done')\n",
    "y_pred_train_ada = section_ada_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_ada = section_ada_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('ada prediction done')\n",
    "y_pred_train_svm = section_svm_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_svm = section_svm_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('svm prediction done')\n",
    "y_pred_train_knn = section_knn_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_knn = section_knn_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('knn prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Set Accuracy: 0.28683943089430897\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.00      0.00      0.00       738\n      hiphop       0.29      1.00      0.45      1131\n         pop       0.00      0.00      0.00       973\n        rock       0.00      0.00      0.00      1094\n\n    accuracy                           0.29      3936\n   macro avg       0.07      0.25      0.11      3936\nweighted avg       0.08      0.29      0.13      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[   0  738    0    0]\n [   2 1129    0    0]\n [   0  972    0    1]\n [   2 1092    0    0]]\n\nTesting Set Accuracy: 0.25888324873096447\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.00      0.00      0.00       199\n      hiphop       0.26      1.00      0.41       255\n         pop       0.00      0.00      0.00       239\n        rock       0.00      0.00      0.00       292\n\n    accuracy                           0.26       985\n   macro avg       0.06      0.25      0.10       985\nweighted avg       0.07      0.26      0.11       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[  0 199   0   0]\n [  0 255   0   0]\n [  0 239   0   0]\n [  0 292   0   0]]\n\nTraining Set Accuracy: 0.506859756097561\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.71      0.64      0.67       738\n      hiphop       0.41      0.64      0.50      1131\n         pop       0.62      0.57      0.59       973\n        rock       0.39      0.22      0.28      1094\n\n    accuracy                           0.51      3936\n   macro avg       0.53      0.52      0.51      3936\nweighted avg       0.51      0.51      0.50      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[473 120  90  55]\n [ 62 727 133 209]\n [ 76 230 554 113]\n [ 58 676 119 241]]\n\nTesting Set Accuracy: 0.4020304568527919\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.64      0.57      0.60       199\n      hiphop       0.33      0.59      0.42       255\n         pop       0.51      0.42      0.46       239\n        rock       0.20      0.11      0.14       292\n\n    accuracy                           0.40       985\n   macro avg       0.42      0.42      0.41       985\nweighted avg       0.40      0.40      0.39       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[113  39  26  21]\n [ 13 150  28  64]\n [ 34  64 101  40]\n [ 17 201  42  32]]\n\nTraining Set Accuracy: 0.6796239837398373\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.83      0.92      0.87       738\n      hiphop       0.57      0.56      0.56      1131\n         pop       0.75      0.91      0.82       973\n        rock       0.58      0.44      0.50      1094\n\n    accuracy                           0.68      3936\n   macro avg       0.68      0.71      0.69      3936\nweighted avg       0.67      0.68      0.67      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[679   8  44   7]\n [ 51 628 126 326]\n [ 35  30 882  26]\n [ 52 427 129 486]]\n\nTesting Set Accuracy: 0.33096446700507615\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.64      0.76      0.70       199\n      hiphop       0.07      0.08      0.08       255\n         pop       0.54      0.61      0.57       239\n        rock       0.04      0.03      0.04       292\n\n    accuracy                           0.33       985\n   macro avg       0.32      0.37      0.35       985\nweighted avg       0.29      0.33      0.31       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[152   6  35   6]\n [ 20  20  46 169]\n [ 45  25 145  24]\n [ 19 221  43   9]]\n\nTraining Set Accuracy: 0.7235772357723578\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.96      0.90      0.93       738\n      hiphop       0.57      0.57      0.57      1131\n         pop       0.94      0.91      0.93       973\n        rock       0.55      0.60      0.57      1094\n\n    accuracy                           0.72      3936\n   macro avg       0.76      0.74      0.75      3936\nweighted avg       0.73      0.72      0.73      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[665  32  10  31]\n [  7 644  21 459]\n [ 10  34 888  41]\n [  9 412  22 651]]\n\nTesting Set Accuracy: 0.11573604060913706\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.40      0.11      0.17       199\n      hiphop       0.03      0.05      0.04       255\n         pop       0.43      0.21      0.29       239\n        rock       0.07      0.10      0.08       292\n\n    accuracy                           0.12       985\n   macro avg       0.23      0.12      0.14       985\nweighted avg       0.21      0.12      0.14       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[ 21  72  36  70]\n [  5  14  16 220]\n [ 24  72  51  92]\n [  3 246  15  28]]\n\n"
    }
   ],
   "source": [
    "def show_results(y_pred_train, y_pred_test):\n",
    "    print(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    print(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    \n",
    "    print(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    print(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "\n",
    "y_pred_trains = [y_pred_train_rf, y_pred_train_ada, y_pred_train_svm, y_pred_train_knn]\n",
    "y_pred_tests = [y_pred_test_rf, y_pred_test_ada, y_pred_test_svm, y_pred_test_knn]\n",
    "\n",
    "for train, test in zip(y_pred_trains, y_pred_tests):\n",
    "    show_results(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}