{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(message):\n",
    "    print(datetime.now().strftime(\"%H:%M:%S -\"), message)\n",
    "    \n",
    "def printnow():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23:28:33 - Reading data from files\n23:28:33 - Done\n\n"
    }
   ],
   "source": [
    "# Create the dictionary of dataframes for training and testing\n",
    "section_headers = ['Intro','Verse','Refrain','Pre-Chorus','Chorus','Post-Chorus','Hooks','Riffs/Basslines','Scratches','Sampling','Bridge','Interlude','Skit','Collision','Instrumental','Solo','Ad-lib','Segue','Outro']\n",
    "header_strip_list = '|'.join(['\\[' + header + '\\]' for header in section_headers])\n",
    "\n",
    "def header_to_filename(train, header):\n",
    "    if train: return 'section_train_test/train_' + header.replace('/', '_').lower() + '.zip'\n",
    "    else: return 'section_train_test/test_' + header.replace('/', '_').lower() + '.zip'\n",
    "\n",
    "log('Reading data from files')\n",
    "train_dfs = {header:pd.read_pickle(header_to_filename(1, header)) for header in section_headers}\n",
    "test_dfs = {header:pd.read_pickle(header_to_filename(0, header)) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "# For dataframes without samples of each genre, add the empty string as lyrics for all genres\n",
    "dummy_df = pd.DataFrame(data={'lyrics' : 8*[''], 'genre' : 2*['country', 'hiphop', 'pop', 'rock']})\n",
    "for header in section_headers:\n",
    "    if len(train_dfs[header]) < 5: train_dfs[header] = pd.concat([train_dfs[header], dummy_df])\n",
    "    if len(test_dfs[header]) < 5: test_dfs[header] = pd.concat([test_dfs[header], dummy_df])\n",
    "    train_dfs[header] = train_dfs[header].reset_index(drop=True)\n",
    "    test_dfs[header] = test_dfs[header].reset_index(drop=True)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23:28:36 - Finding the counts of each section type in the corpus\n1 Intro: 1056\n2 Verse: 11248\n3 Refrain: 438\n4 Pre-Chorus: 4128\n5 Chorus: 13602\n6 Post-Chorus: 1029\n7 Hooks: 16\n8 Riffs/Basslines: 16\n9 Scratches: 16\n10 Sampling: 16\n11 Bridge: 3417\n12 Interlude: 259\n13 Skit: 17\n14 Collision: 18\n15 Instrumental: 190\n16 Solo: 177\n17 Ad-lib: 19\n18 Segue: 16\n19 Outro: 2332\n23:28:36 - Done: total length is 38010\n\n23:28:36 - Finding the percent frequencies of each section type in the corpus\n1 Intro: 0.027782162588792424\n2 Verse: 0.2959221257563799\n3 Refrain: 0.011523283346487766\n4 Pre-Chorus: 0.10860299921073402\n5 Chorus: 0.3578531965272297\n6 Post-Chorus: 0.02707182320441989\n7 Hooks: 0.0004209418574059458\n8 Riffs/Basslines: 0.0004209418574059458\n9 Scratches: 0.0004209418574059458\n10 Sampling: 0.0004209418574059458\n11 Bridge: 0.0898973954222573\n12 Interlude: 0.006813996316758748\n13 Skit: 0.00044725072349381743\n14 Collision: 0.00047355958958168905\n15 Instrumental: 0.004998684556695606\n16 Solo: 0.004656669297553276\n17 Ad-lib: 0.0004998684556695607\n18 Segue: 0.0004209418574059458\n19 Outro: 0.0613522757169166\n23:28:36 - Done\n\n"
    }
   ],
   "source": [
    "log('Finding the counts of each section type in the corpus')\n",
    "total_length = 0\n",
    "for i,header in enumerate(section_headers):\n",
    "    length = len(train_dfs[header]) + len(test_dfs[header])\n",
    "    print(i + 1, header + ':', length)\n",
    "    total_length += len(train_dfs[header]) + len(test_dfs[header])\n",
    "log(f'Done: total length is {total_length}\\n')\n",
    "\n",
    "log('Finding the percent frequencies of each section type in the corpus')\n",
    "corpus_weights = {}\n",
    "for i,header in enumerate(section_headers):\n",
    "    frequency = (len(train_dfs[header]) + len(test_dfs[header])) / total_length\n",
    "    print(i + 1, header + ':', frequency)\n",
    "    corpus_weights[header] = frequency\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#get one array of sections \n",
    "train_arr = [lyric for df in train_dfs.values() for lyric in df['lyrics']]\n",
    "\n",
    "vectorizer_OH = CountVectorizer(min_df = 3, stop_words = 'english')\n",
    "vectorizer_OH.fit(train_arr)\n",
    "vocab_size = len(vectorizer_OH.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:47:02 - Encoding the training data\n01:47:04 - Done\n\n01:47:04 - Encoding the testing data\n01:47:04 - Done\n\n"
    }
   ],
   "source": [
    "# Create TFIDF up to trigram encoding for nonredundant data\n",
    "def encode_OH(df): \n",
    "    #vocab_length = len(vectorizer_OH.get_feature_names())\n",
    "    encoding_OH = vectorizer_OH.transform(df['lyrics'].values)\n",
    "    df_OH = pd.DataFrame(encoding_OH.toarray())\n",
    "    df_OH['y'] = df['genre']\n",
    "    return df_OH\n",
    "\n",
    "# Create the training and testing encoded dataframes\n",
    "log('Encoding the training data')\n",
    "train_OH = {header:encode_OH(train_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    "\n",
    "log('Encoding the testing data')\n",
    "test_OH = {header:encode_OH(test_dfs[header]) for header in section_headers}\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:36:26 - Loading the raw string data\n01:36:26 - Done\n\n01:36:26 - Splitting the raw string data\n01:36:26 - Done\n\n"
    }
   ],
   "source": [
    "# Load the string lyrics data to make predictions\n",
    "log('Loading the raw string data')\n",
    "string_train = pd.read_pickle(r'train_test_data/train.zip')\n",
    "string_test = pd.read_pickle(r'train_test_data/test.zip')\n",
    "log('Done\\n')\n",
    "\n",
    "# splits the given lyrics by section \n",
    "def split_by_section(lyrics):\n",
    "    headers = [word[1:-1] for word in lyrics.split() if word[0] == '[' and word[-1] == ']' and word[1:-1] in section_headers]\n",
    "    split_sections = re.split(header_strip_list, lyrics)\n",
    "    ret_sections = []\n",
    "    for section in split_sections:\n",
    "        mod_section = section.replace('[END]','').replace('[START]','').strip()\n",
    "        if not(mod_section in ['', ' ','\\n']): ret_sections.append(mod_section)\n",
    "    return list(zip(headers, ret_sections))\n",
    "\n",
    "# Turn the raw string data into tuples of section strings and the lyrics of that section\n",
    "log('Splitting the raw string data')\n",
    "split_string_train = string_train\n",
    "split_string_train['lyrics'] = string_train['lyrics'].map(split_by_section)\n",
    "split_string_test = string_test\n",
    "split_string_test['lyrics'] = string_test['lyrics'].map(split_by_section)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:36:34 - Splitting the encoding string lyrics of each section\n01:36:34 - \tTesting set\n01:36:35 - \tTraining set\n01:36:36 - ValueError0\t[]\n01:36:36 - ValueError0\t[]\n01:36:38 - ValueError0\t[]\n01:36:38 - ValueError0\t[]\n01:36:38 - Done\n\n"
    }
   ],
   "source": [
    "# Encode the split string data with TFIDF (up to trigrams) and SVD \n",
    "def split_lyrics_encode(split_lyrics):\n",
    "    sections_array = [section for section,lyrics in split_lyrics]\n",
    "    lyrics_array = [lyrics + ' ' for section,lyrics in split_lyrics]\n",
    "    try:\n",
    "        vec_OH = vectorizer_OH.transform(lyrics_array).toarray()\n",
    "        assert len(vec_OH) == len(sections_array)\n",
    "        assert len(vec_OH[0]) == vocab_size\n",
    "        return list(zip(sections_array, vec_OH))\n",
    "    except IndexError:\n",
    "        log('ValueError' + str(len(sections_array)) + '\\t' + str([len(lyrics) for lyrics in lyrics_array]))\n",
    "        return list(zip(sections_array, np.array([[0]*vocab_size]*len(split_lyrics))))\n",
    "\n",
    "# Turn the split string data into tuples of section strings and TFIDF encoded lyrics of that section\n",
    "log('Splitting the encoding string lyrics of each section')\n",
    "log('\\tTesting set')\n",
    "split_encoded_test = split_string_test\n",
    "split_encoded_test['lyrics'] = split_string_test['lyrics'].map(split_lyrics_encode)\n",
    "log('\\tTraining set')\n",
    "split_encoded_train = split_string_train\n",
    "split_encoded_train['lyrics'] = split_string_train['lyrics'].map(split_lyrics_encode)\n",
    "log('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:37:18 - Saving split encoded lyrics as pickles\n01:38:07 - Done\n\n"
    }
   ],
   "source": [
    "log('Saving split encoded lyrics as pickles')\n",
    "split_encoded_train.to_pickle('section_train_test/OH_encoded_train.zip')\n",
    "split_encoded_test.to_pickle('section_train_test/OH_encoded_test.zip')\n",
    "log('Done\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "02:50:59 - Reading split encoded lyrics pickles into dataframes\n"
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-d05ed644462e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading split encoded lyrics pickles into dataframes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msplit_encoded_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'section_train_test/OH_encoded_train.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msplit_encoded_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'section_train_test/OH_encoded_test.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mpeek\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log('Reading split encoded lyrics pickles into dataframes')\n",
    "split_encoded_train = pd.read_pickle('section_train_test/OH_encoded_train.zip')\n",
    "split_encoded_test = pd.read_pickle('section_train_test/OH_encoded_test.zip')\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionSplitClassifier:\n",
    "    def __init__(self, section_classifiers, weights):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def set_section_classifiers(self, section_classifiers):\n",
    "        if section_classifiers:\n",
    "            try: self.section_classifiers = {key:value for key,value in section_classifiers.items()}\n",
    "            except AttributeError: raise ValueError('section_classifiers was not a dictionary')\n",
    "        else: raise ValueError('section_classifiers was None')\n",
    "\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        if weights:\n",
    "            if np.round(np.sum([weight for weight in weights.values()])) == 1:\n",
    "                try: self.weights = {key:value for key,value in weights.items()}\n",
    "                except AttributeError: raise ValueError('weights was not a dictionary')\n",
    "            else: raise ValueError('weights did not sum to 1')\n",
    "        else: raise ValueError('weights was None')\n",
    "\n",
    "\n",
    "    def fit(self, X, y, section, verbose=0):\n",
    "        if verbose: print(f'Training {section}...')\n",
    "        self.section_classifiers[section] = self.section_classifiers[section].fit(X,y)\n",
    "        if verbose: print(f'Done training {section}')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        def predict_section(self, X_section, section):\n",
    "            def str_to_array(string, weight):\n",
    "                return float(weight) * np.array([\n",
    "                    int(string == 'country'), \n",
    "                    int(string == 'hiphop'),\n",
    "                    int(string == 'pop'), \n",
    "                    int(string == 'rock')])\n",
    "            return [str_to_array(pred, self.weights[section]) for pred in self.section_classifiers[section].predict([X_section])]\n",
    "\n",
    "        def predict_song(self, split_encoded):\n",
    "            def array_to_str(array):\n",
    "                return ['country','hiphop','pop','rock'][np.argmax(array)]\n",
    "            pred = np.array([0,0,0,0])\n",
    "            for section,encoding in split_encoded:\n",
    "                pred = np.sum([pred, predict_section(self, encoding, section)], axis=0)\n",
    "            return array_to_str(pred)\n",
    "\n",
    "        preds = [predict_song(self, lyrics) for lyrics in X]\n",
    "        assert len(preds) == len(X)\n",
    "        return(preds)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the different kinds of section classifiers and their respective song classifiers\n",
    "section_rf_classifiers = {\n",
    "    header:RandomForestClassifier(criterion='entropy', ccp_alpha=0.0175) for header in section_headers}\n",
    "section_rf_classifier = SectionSplitClassifier(section_rf_classifiers, corpus_weights)\n",
    "\n",
    "section_ada_classifiers = {\n",
    "    header:OneVsRestClassifier(AdaBoostClassifier(), n_jobs=-1) for header in section_headers}\n",
    "section_ada_classifier = SectionSplitClassifier(section_ada_classifiers, corpus_weights)\n",
    "\n",
    "section_svm_classifiers = {\n",
    "    header:OneVsRestClassifier(SVC(kernel=\"linear\", C=0.025), n_jobs=-1) for header in section_headers}\n",
    "section_svm_classifier = SectionSplitClassifier(section_svm_classifiers, corpus_weights)\n",
    "\n",
    "section_knn_classifiers = {\n",
    "    header:OneVsOneClassifier(KNeighborsClassifier(3)) for header in section_headers}\n",
    "section_knn_classifier = SectionSplitClassifier(section_knn_classifiers, corpus_weights)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "01:47:21 - Training Random Forest classifier with all sections\n01:52:49 - Done\n\n01:52:49 - Training ADA Boost classifier with all sections\n02:02:49 - Done\n\n02:02:49 - Training Linear SVM classifier with all sections\n02:41:38 - Done\n\n02:41:38 - Training k-Nearest Neighbors classifier with all sections\n02:45:09 - Done\n\n"
    }
   ],
   "source": [
    "# train each kind of song classifier\n",
    "log('Training Random Forest classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_rf_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training ADA Boost classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_ada_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training Linear SVM classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_svm_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    "\n",
    "log('Training k-Nearest Neighbors classifier with all sections')\n",
    "for header in section_headers:\n",
    "    section_knn_classifier.fit(train_OH[header].drop(columns=['y']), train_OH[header]['y'], header, verbose=0)\n",
    "log('Done\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "02:45:49 - start\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 8642 and input n_features is 500 ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-f5e047d6f99b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# predict the genres of both training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_pred_train_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msection_rf_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_encoded_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lyrics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred_test_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msection_rf_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_encoded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lyrics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rf prediction done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-457ddfedce3f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marray_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlyrics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlyrics\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-457ddfedce3f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marray_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlyrics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlyrics\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-457ddfedce3f>\u001b[0m in \u001b[0;36mpredict_song\u001b[1;34m(self, split_encoded)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit_encoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_section\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marray_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-457ddfedce3f>\u001b[0m in \u001b[0;36mpredict_section\u001b[1;34m(self, X_section, section)\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     int(string == 'rock')])\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msection_classifiers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_section\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mpredict_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    389\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 8642 and input n_features is 500 "
     ]
    }
   ],
   "source": [
    "# predict the genres of both training and testing sets\n",
    "log('start')\n",
    "y_pred_train_rf = section_rf_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_rf = section_rf_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('rf prediction done')\n",
    "y_pred_train_ada = section_ada_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_ada = section_ada_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('ada prediction done')\n",
    "y_pred_train_svm = section_svm_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_svm = section_svm_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('svm prediction done')\n",
    "y_pred_train_knn = section_knn_classifier.predict(split_encoded_train['lyrics'])\n",
    "y_pred_test_knn = section_knn_classifier.predict(split_encoded_test['lyrics'])\n",
    "log('knn prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Set Accuracy: 0.5111788617886179\nClassification Report Training Set:\n              precision    recall  f1-score   support\n\n     country       0.72      0.29      0.41       738\n      hiphop       0.48      0.85      0.61      1131\n         pop       0.51      0.86      0.64       973\n        rock       0.64      0.01      0.02      1094\n\n    accuracy                           0.51      3936\n   macro avg       0.59      0.50      0.42      3936\nweighted avg       0.58      0.51      0.42      3936\n\n--------------------\nConfusion Matrix Training Set:\n[[211  41 486   0]\n [ 12 958 157   4]\n [ 59  79 834   1]\n [ 11 919 155   9]]\n\nTesting Set Accuracy: 0.46294416243654823\nClassification Report Testing Set:\n              precision    recall  f1-score   support\n\n     country       0.71      0.24      0.36       199\n      hiphop       0.42      0.83      0.56       255\n         pop       0.48      0.82      0.60       239\n        rock       0.00      0.00      0.00       292\n\n    accuracy                           0.46       985\n   macro avg       0.40      0.47      0.38       985\nweighted avg       0.37      0.46      0.36       985\n\n--------------------\nConfusion Matrix Testing Set:\n[[ 48  13 138   0]\n [  0 212  38   5]\n [ 19  24 196   0]\n [  1 251  40   0]]\n\n"
    }
   ],
   "source": [
    "def show_results(y_pred_train, y_pred_test):\n",
    "    print(f'Training Set Accuracy: {accuracy_score(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    print(f'Classification Report Training Set:\\n{classification_report(split_encoded_train[\"genre\"], y_pred_train)}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Training Set:\\n{confusion_matrix(split_encoded_train[\"genre\"], y_pred_train)}\\n')\n",
    "    \n",
    "    print(f'Testing Set Accuracy: {accuracy_score(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    print(f'Classification Report Testing Set:\\n{classification_report(split_encoded_test[\"genre\"], y_pred_test)}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Testing Set:\\n{confusion_matrix(split_encoded_test[\"genre\"], y_pred_test)}\\n')\n",
    "",
    "y_pred_trains = [y_pred_train_rf, y_pred_train_ada, y_pred_train_svm, y_pred_train_knn]\n",
    "y_pred_tests = [y_pred_test_rf, y_pred_test_ada, y_pred_test_svm, y_pred_test_knn]\n",
    "",
    "for train, test in zip(y_pred_trains, y_pred_tests):\n",
    "    show_results(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}